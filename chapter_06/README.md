# Chapter 06. 카프카 내부 메커니즘
## 6.1 클러스터 멤버십
* 카프카는 현재 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 아파치 주키퍼를 사용한다.
* 각 브로커는 브로커 설정 파일에 정의되었거나 아니면 자동으로 생성된 고유한 식별자를 가진다.
* 컨트롤러를 포함한 카프카 브로커들과 몇몇 생태계 툴들은 브로커가 등록되는 주키퍼의 /brokers/ids 경로를 구독함으로써 브로커가 추가되거나 제거될 때마다 알림을 받는다.
* 브로커가 정지하면 브로커를 나타내는 ZNode 역시 삭제되지만, 브로커의 ID는 다른 자료구조에 남아 있게 된다.
  * 각 토픽의 레플리카 목록에는 해당 레플리카를 저장하는 브로커의 ID가 포함된다.
  * 만약 특정한 브로커가 완전히 유실되어 동일한 ID를 가진 새로운 브로커를 투입할 경우, 클러스터에서 유실된 브로커의 자리를 대신해서 이전 브로커의 토픽과 파티션들을 할당받는다.

## 6.2 컨트롤러
* 컨트롤러는 일반적인 카프카 브로커의 기능에 더해서 파티션 리더를 선출하는 역할을 추가적으로 맡는다.
* 클러스터에서 가장 먼저 시작되는 브로커는 주키퍼의 /controller에 Ephemeral 노드를 생성함으로써 컨트롤러가 된다.
* 브로커들은 주키퍼의 컨트롤러 노드에 변동이 생겼을 때 알림을 받기 위해서 이 노드에 와치를 설정한다.
* 컨트롤러 브로커가 멈추거나 주키퍼와의 연결이 끊어질 경우, 이 Ephemeral 노드는 삭제된다.
  * 컨트롤러가 사용하는 주키퍼 클라이언트가 zookeeper.session.timeout.ms 에 설정된 값보다 오랫동안 주키퍼에 하트비트를 전송하지 않는 경우에도 해당된다.
* 컨트롤러 안의 다른 브로커들은 주키퍼에 설정된 와치를 통해서 컨트롤러가 없어졌다는 것을 알아차리게 되며 주키퍼에 컨트롤러 노드를 생성하려고 시도하게 된다.
* 주키퍼에 가장 먼저 새로운 노드를 생성하는 데 성공한 브로커가 다음 컨트롤러가 되고, 나머지 브로커들은 새 컨트롤러 노드에 대한 와치를 다시 생성한다.
* 브로커는 새로운 컨트롤러가 선출될 때마다 주키퍼의 조건적 증가 연산에 의해 증가된 에포크(epoch) 값을 전달받게 된다.
* 브로커는 현재 컨트롤러의 에포크 값을 알고 있기 때문에 더 낮은 에포크 값을 가진 컨트롤러로부터 메시지를 받을 경우 무시한다.
  * 컨트롤러 브로커가 오랫동안 가비지 수집 때문에 멈춘 사이에 주키퍼와 연결이 끊어질 수 있고, 그 사이에 새 컨트롤러가 선출될 수 있기 때문이다.
  * 이전 컨트롤러가 작업을 재개할 경우, 새로운 컨트롤러가 선출되었다는 것을 알지 못한 채 브로커에 메시지를 보낼 수 있다.
* 브로커가 클러스터를 나갔다는 사실을 컨트롤러가 알아차리면, 컨트롤러는 해당 브로커가 리더를 맡고 있던 모든 파티션에 대해 새로운 브로커를 할당해준다.
* 새로 리더가 된 브로커 각각은 클라이언트로부터의 쓰기 혹은 읽기 요청을 처리하기 시작하고, 팔로워들은 새 리더로부터 메시지를 복제하기 시작한다.
* 브로커에 속한 모든 레플리카들은 팔로워로 시작하며, 리더로 선출될 자격을 얻기 위해서는 그 전에 리더에 쓰여진 메시지를 따라잡아야 한다.

### 6.2.1 KRaft: 카프카의 새로운 래프트 기반 컨트롤러
* 3.3부터 카프카 클러스터는 전통적인 주키퍼 기반 컨트롤러와 KRaft 기반 컨트롤러 둘 중 하나와 함께 실행될 수 있게 되었다.
* 카프카 컨트롤러가 주키퍼로에서 KRaft로 교체하기로 결정한 이유
  * 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적으로 이루어지지만, 브로커 메시지를 보내는 작업은 비동기적으로 이루어진다. 주키퍼로부터 업데이트를 받는 과정 역시 비동기적으로 이루어진다. 그렇기 때문에 브로커, 컨트롤러, 주키퍼 간에 메타데이터 불일치가 발생할 수 있다.
  * 컨트롤러가 재시작될 때마다 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터를 읽어와야 한다. 그리고 나서 이 메타데이터를 모든 브로커로 전송하는데, 이 부분이 병목이다. 
  * 주키퍼는 그 자체로 분산 시스템이며, 카프카와 마친가지로 운영을 위해서는 어느 정도 기반 지식이 있어야 하기 때문에 개발자들은 두 개의 분산 시스템에 대해 배워야 한다.
* 새로운 컨트롤러 설계의 핵삼 아이디어는 카프카 그 자체에 사용자가 상태를 이벤트 스트림으로 나타낼 수 있도록 로그 기반 아키텍처를 도입한다는 점이다.
  * 다수의 컨슈머를 사용해서 이벤트를 재생함으로써 최신 상태를 빠르게 따라잡을 수 있다. 
  * 로그는 이벤트 사이에 명확한 순서를 부여하며, 컨슈머들이 항상 하나의 타임라인을 따라 움직이도록 보장한다.
* 래프트 알고리즘을 사용함으로써, 컨트롤러 노드들은 외부 시스템에 의존하지 않고 자체적으로 리더를 선출할 수 있게 되었다.
* 메타데이터 로그의 리더 역할을 맡고 있는 컨트롤러는 액티브 컨트롤러라고 부른다.
  * 액티브 컨트롤러는 브로커가 보내온 RPC 호출을 처리한다.
  * 팔로워 컨트롤러들은 액티브 컨트롤러에 쓰여진 데이터들을 복제하며, 액티브 컨트롤러에 장애가 발생했을 시에 즉시 투입될 수 있도록 준비 상태를 유지한다.
  * 컨트롤러들이 모두 최신 상태를 가지고 있으므로, 컨트롤러 장애 복구는 모든 상태를 새 컨트롤러로 이전하는 리로드 기간을 필요로 하지 않는다.
* 컨트롤러는 다른 브로커에 변경 사항을 Push하지 않고, 다른 브로커들이 새로 도입된 MetadataFetch API 를 사용해서 액티브 컨트롤러로부터 변경 사항을 Pull 한다.

## 6.3 복제
* 카프카는 분산 시스템으로, 개별 노드에 장애가 발생할 수 있기 때문에 신뢰성과 지속성을 보장하기 위해서 복제는 필수적이다.
* 카프카는 토픽을 단위로 데이터를 조직화하고, 각 토픽윽은 1개 이상의 파티션으로 분할되며, 각 파티션은 다수의 레플리카를 가질 수 있다. 그리고 각각의 레플리카는 브로커에 저장된다.
* 레플리카는 리더와 팔로워로 구분된다.
  * 리더 레플리카
    * 각 파티션에는 리더 역할을 하는 레플리카가 하니씩 있다.
    * 일관성 보장을 위해, 모든 쓰고 요청은 리더 레플리카로 주어진다.
    * 클라이언트들은 리더 레플리카나 팔로워로부터 레코드를 읽어올 수 있다.
  * 팔로워 레플리카
    * 파티션에 속한 모든 레플리카 중에서 리더 레플리카를 제외한 나머지를 팔로워 레플리카라고 한다.
    * 별도로 설정을 잡아주지 않는 한, 팔로워는 클라이언트의 요청을 처리할 수 없다.
    * 팔로워 레플리카는 리더 레플리카로부터 들어온 최근 메시지들을 복제함으로써 최신 상태를 유지한다.
    * 해당 파티션의 리더 레플리카에 이상이 생길 경우, 팔로워 레플리카 중 하나가 파티션의 새 리더 피타션으로 승격된다.
* 리더 레플리카 수행하는 또 다른 일은 어느 팔로워 레플리카가 리더 레플리카의 최신 상태를 유지하고 있는지를 확인하는 것이다.
* 팔로워 레플리카는 리더 레플리카로부터 메시지를 복제하여 최신 상태를 유지하도록 하지만, 다양한 원인으로 인해 동기화가 깨질 수 있다.
* 팔로워 레플리카는 동기화를 유지하기 위해 리더 레플리카에 읽기 요청을 보내고, 리더 레플리카는 메시지를 전송한다.
  * 읽기 요청에 대한 메시지는 복제를 수행하는 입장에서 다음번에 받아야 할 메시지 오프셋을 포함할 뿐만 아니라 언제나 메시지를 순서대로 되돌려준다.
  * 리더 레플리카 입장에서는 팔로워 레플리카가 요청한 마지막 메시지까지 복제를 완료했는지, 이후 새로 추가된 메시지가 없는지의 여부를 알 수 있게 된다.
  * 팔로워가 10초이상 메시지 요청을 보내지 않거나 10초 이상 가장 최근의 메시지를 가져가지 않을 경우 해당 레플리카는 동기화가 풀린 것으로 간주된다. (out-of-sync replica)
  * 지속적으로 최신 메시지를 요청하고 있는 레플리카는 in-sync replica 라고 부른다. 리더에 장애가 발생한 경우 인-싱크 레플리카만 파티션 리더로 선출될 수 있다.
  * out-of-sync replica 로 판정되기 전, 비활성 상태이거나 뒤쳐진 상태일 수 있는 시간은 replica.lag.time.max.ms 설정 매개변수에 의해 결정된다.
  * 허용될 수 있는 랙의 양은 클라이언트의 작동이나 리더 선출 과정에 있어서의 데이터 보존에 영향을 미친다.
* 각 파티션은 preferred leader 를 갖는다. preferred leader 는 토픽이 처음 생성되었을 때 리더였던 레플리카를 가리킨다.
  * 파티션이 처음 생성되었던 시점에서 리더 레플리카가 모든 브로커에 걸쳐 균등하게 분포되기 때문에 preferred 라는 표현이 붙었다.
  * 결과적으로는 클러스터 내의 모든 파티션에 대해 preferred leader 가 실제 리더가 될 경우 부하가 브로커 사이에 균등하게 분배될 것이라고 예상할 수 있다.
  * auto.leader.rebalance.enable=true 설정이 기본적으로 잡혀 있는데, 이 설정은 선호 리더가 현재 리더가 아니지만, 현재 리더와 동기화가 되고 있을 경우 리더 선출을 실행하여 선호 리더를 현재 리더로 만든다.

## 6.4 요청 처리
* 브로커는 연결을 받는 각 포트별로 acceptor 스레드를 하나씩 실행시킨다.
* acceptor 스레드는 연결을 생성하고 들어온 요청을 processor 스레드에 넘겨 처리하도록 한다.
* processor 스레드(네트워크 스레드라고 부르기도 한다)의 수는 설정이 가능하다.
* 네트워크 스레드는 클라이언트 연결로부터 들어온 요청들을 받아서 요청 큐에 넣고, 응답 큐에서 응답을 가져다 클라이언트로 보낸다.
* 쓰기 요청과 읽기 요청은 모두 파티션의 리더 레플리카로 전송되어야 한다.
* 카프카 클라이언트는 메타데이터 요청이라 불리는 유형의 요청을 사용하여, 각 파티션의 리더 레플리카가 어디 있는지 알 수 있다.
  * 메타데이터 요청은 아무 브로커에 보내도 상관 없는데, 모든 브로커들이 이러한 메타데이터를 캐싱하고 있기 때문이다.
  * 클라이언트는 이 정보를 캐싱해 두었다가 이 정보를 사용해서 각 파티션의 리더 역할을 맡고 있는 브로커에 바로 쓰거나 읽는다.
  * 토픽의 메타데이터가 변경될 수 있기 때문에 이를 refresh 해야하는데, metadata.max.age.ms 설정 매개변수로 간격을 조절할 수 있다.
  * 또 다른 경우로, 클라이언트가 Not a Leader 에러를 리턴받을 경우 요청을 재시도하기 전에 메타데이터를 먼저 refresh한다.

### 6.4.1 쓰기 요청
* acks 설정 매개변수는 쓰기 작업이 성공한 것으로 간주되기 전 메시지에 응답을 보내야 하는 브로커의 수를 가리킨다.
  * acks=1 : 리더만 메시지를 받았을 때
  * acks=all : 모든 in-sync replica 가 메시지를 받았을 때
  * acks=0 : 메시지가 보내졌을 때, 즉, 브로커의 응답을 기다리지 않는다.
* 파티션의 리더 레플리카를 가지고 있는 브로커가 해당 파티션에 대한 쓰기 요청을 받게 되면 몇 가지 유효성 검증부터 한다.
  * 데이터를 보내고 있는 사용자가 토픽에 대한 쓰기 권한을 가지고 있는지
  * 요청에 지정되어 있는 acks 설정값이 올바른지
  * acks 설정값이 all로 잡혀 있을 경우, 메시지를 안전하게 쓸 수 있을 만큼 충분한 in-sync replica 가 있는지
* 리눅스의 경우 메시지는 파일시스템 캐시에 쓰여지는데, 이들이 언제 디스크에 반영될지는 보장이 없다. 카프카는 데이터가 디스크에 저장될 때까지 기다리지 않고, 메시지의 지속성을 위해 복제에 의존한다.

### 6.4.2 읽기 요청
* 클라이언트는 브로커에 토픽, 파티션, 오프셋 목록에 해당하는 메시지들을 보내 달라는 요청을 보낸다.
  * ex) Test 토픽의 파티션 0, 오프셋 53부터의 메시지를 보내주세요.
* 클라이언트는 각 파티션에 대해 브로커가 리턴할 수 있는 최대 데이터의 양을 지정한다.
  * 클라이언트는 브로커가 되돌려준 응답을 담을 수 있을 정도로 충분히 큰 메모리를 할당해야 하기 때문이다.
  * 한도값이 없을 경우, 브로커는 클라이언트가 메모리 부족에 처할 수 있을 정도로 큰 응답을 보낼 수도 있다.
* 카프카는 제로카피(zero-copy) 최적화를 적용하여 메시지를 보낸다.
  * 파일(리눅스의 파일시스템 캐시)에서 읽어 온 메시지들을 중간 버퍼를 거치지 않고 바로 네트워크 채널로 보낸다.
  * 클라이언트에게 데이터를 보내기 전에 로컬 캐시에 저장하지 않기 때문에 데이터를 복사하고 메모리 상에 버퍼를 관리하기 위한 오버헤드가 사라진다.
* 클라이언트는 브로커가 리턴할 수 있는 데이터 양의 상한을 지정하는 것에 더해서, 리턴될 데이터의 하한도 지정할 수 있다.
  * 하한이 10K로 잡혀있다면, 브로커가 보낼 데이터가 최소한 10K 바이트가 쌓이면 리턴하도록 요청한다.
  * 클라이언트가 트래픽이 그리 많지 않은 토픽들로부터 메시지를 읽어오고 있을 때 CPU와 네트워크 사용량을 감소시키는 좋은 방법이다.
  * 브로커가 충분한 데이터를 가질 때까지 클라이언트가 계속 기다리는 것은 아니고, 시간이 조금 지난 후에는 있는 데이터라도 가져다 처리할 수 있도록 타임아웃을 지정할 수 있다.
* 클라이언트는 모든 in-sync replica 에 쓰여진 메시지들만 읽을 수 있다.
  * 충분한 수의 레플리카에 복제가 완료되지 않는 메시지는 불안전한 것으로 간주되기 때문이다.
  * 복제가 충분히 되지 않은 상황에서 리더에 장애가 발생하여 다른 레플리카가 리더 역할을 이어받는다면, 이 메시지들은 더 이상 카프카에 존재하지 않게 된다.
  * 만약 리더에만 존재하는 메시지를 읽을 수 있도록 한다면, 장애가 발생하는 상황에서 일관성이 결여될 수 있게 되는 것이다.
  * 컨슈머가 어떤 메시지를 읽은 상태에서 리더 브로커에 장애가 발생하고, 다른 브로커에 해당 메시지가 복제된 적이 없다면, 컨슈머들이 읽어 온 메시지들 사이에 불일치가 발생한다.
  * 메시지 복제가 완료될 때까지 기다려야 하므로, 브로커 사이의 복제가 늦어지면 새 메시지가 컨슈머에 도달하는 데 걸리는 시간도 길어지게 된다.
  * 지연되는 시간은 replica.lag.time.max.ms 설정값에 따라 제한되는데, in-sync replica 상태로 판정되는 레플리카가 새 메시지를 복제하는 과정에서 지연될 수 있는 최대 시간이며, 이 시간 이상으로 지연되면 out-of-sync replica 가 된다.

## 6.5 물리적 저장소
### 6.5.1 계층화된 저장소
* 카프카는 대량의 데이터를 저장히기 위한 목적으로 사용되고 있는데, 이것은 아래와 같은 문제를 야기한다.
  * 파티션별로 저장 가능한 데이터에는 한도가 있다. 최대 보존 기한과 파티션 수는 제품의 요구 조건이 아닌 물리적인 디스크 크기에도 제한을 받는다.
  * 디스크와 클러스터 크기는 저장소 요구 조건에 의해 결정된다. 지연과 처리량이 주 고려사항일 경우 클러스터는 필요한 것 이상으로 커지는 경우가 많다. 이는 곳 비용으로 직결된다.
  * 클러스터의 크기를 키우거나 줄일 때, 파티션의 위치를 다른 브로커로 옮기는 데 걸리는 시간은 파티션의 수에 따라 결정된다. 파티션의 크기가 클수록 클러스터의 탄력성은 줄어든다.
* 계층화된 저장소 기능에서 카프카 클러스터의 저장소를 로컬과 원격 두 계층으로 나눌 수 있다.
  * 로컬 계층은 현재의 카프카 저장소 계층과 똑같이 로컬 세그먼트를 저장하기 위해 카프카 브로커의 로컬 디스크를 사용한다.
  * 원격 계층은 완료된 로그 세그먼트를 저장하기 위해 HDFS 나 S3 와 같은 전용 저장소 시스템을 사용한다.
* 사용자는 계층별로 서로 다른 보존 정책을 설정할 수 있는데, 로컬 저장소가 리모트 계층 저장소에 비해 훨씬 비싼 것이 일반적이므로 로컬 계층의 보존 기한은 짧게, 원격 계층의 보존 기한은 그보다 길게 설정한다.
* 로컬 저장소는 원격 저장소에 비해 지연이 훨씬 짧다.
* 계층화된 저장소 기능으로 인해 카프카 브로커에 로컬 저장되는 데이터의 양이 줄어들며, 복구와 리밸런싱 과정에서 복사되어야 할 데이터의 양 역시 줄어든다.
* 원격 계층에 저장되는 로그 세그먼트들은 굳이 브로커로 복원될 필요 없이 원격 계층에서 바로 클라이언트로 전달된다.

### 6.5.2 파티션 할당
* 토픽을 생성하면, 카프카는 파티션을 브로커 중 하나에 할당한다.
  * 만약 브로커가 6개 있고, 파티션이 10개, 복제 팩터가 3인 토픽을 생성하기로 했다면, 카프카는 30개의 파티션 레플리카를 6개의 브로커에 할당해 주어야 한다.
* 파티션 할당의 목표는 다음과 같다.
  * 레플리카들을 가능한 한 브로커 간에 고르게 분산시킨다.
  * 각 파티션에 대해 각각의 레플리카는 서로 다른 브로커에 배치되도록 한다.
  * 브로커에 랙 정보가 설정되어 있다면, 가능한 한 각 파티션의 레플리카들을 서로 다른 랙에 할당한다. 이렇게 하면 랙 전체가 작동 불능에 빠지더라도 파티션 전체가 사용 불능에 빠지는 사태를 방지할 수 있다.
* 각 파티션과 레플리카에 올바른 브로커를 선택했다면, 새 파티션을 저장할 디렉토리를 결정해야 한다.
  * 이 작업은 파티션별로 독립적으로 수행되며, 각 디렉토리에 저장되어 있는 파티션의 수를 센 뒤, 가장 적은 파티션이 저장된 디렉토리에 새 파티션을 저장한다.
  * 이는 만약 새로운 디스크를 추가할 경우, 모든 새 파티션들은 이 디스크에 생성될 것이라는 것을 의미한다. 균형이 잡히기 전까지는 새 디스크가 항상 가장 적은 수의 파티션을 보유하기 때문이다.

### 6.5.3 파일 관리
* 카프카는 영구히 데이터를 저장하지 않고, 데이터를 지우기 전에 모든 컨슈머들이 메시지를 읽어갈 수 있도록 기다리지도 않는다.
* 카프카는 각각의 토픽에 대해 보존 기한(retention period)을 설정할 수 있다.
* 큰 파일에서 삭제해야 할 메시지를 찾아서 지우는 작업은 시간이 오래 걸리고, 에러의 가능성도 높기 때문에 하나의 파티션을 여러 개의 세그먼트로 분할하여 저장한다.
  * 기본적으로, 각 세그먼트는 1GB의 데이터 혹은 최근 1주일치의 데이터 중 적은 쪽만큼을 저장한다.
  * 파티션 단위로 메시지를 쓰는 만큼 각 세그먼트 한도가 다 차면 세그먼트를 닫고 새 세그먼트를 생성한다.
  * 현재 쓰여지고 있는 세그먼트를 액티브 세그먼트라고 한다.

### 6.5.4 파일 형식
* 각 세그먼트는 하나의 데이터 파일 형태로 저장된다.
* 파일 안에는 카프카의 메시지와 오프셋이 저장된다.
* 디스크에 저장되는 데이터의 형식은 사용자가 프로듀서를 통해서 브로커로 보내고, 나중에 브로커로부터 컨슈머로 보내지는 메시지의 형식과 동일하다.
  * 네트워크를 통해 전달되는 형식과 디스크에 저장되는 형식을 통일함으로써 카프카는 컨슈머에게 메시지를 전송할 때 제로카피 최적화를 달성할 수 있다.
  * 메시지 형식을 변경하고자 한다면, 네트워크 프로토콜과 디스크 저장 형식이 모두 변경되어야 하며 카프카 브로커들은 업그레이드로 인해 2개의 파일 형식이 뒤섞여 있는 파일을 처리할 방법을 알아야 한다.
* 카프카 메시지는 사용자 페이로드와 시스템 헤더, 두 부분으로 나누어진다.
  * 사용자 페이로드는 키값(Optional)과 밸류값, 헤더(Optional) 모음이 포함된다.

### 6.5.5 인덱스
* 카프카는 컨슈머가 임의의 사용 가능한 오프셋에서부터 메시지를 읽어오기 시작할 수 있다.
* 브로커가 주어진 오프셋의 메시지를 빠르게 찾을 수 있도록 하기 위해 카프카는 각 파티션에 대해 오프셋을 유지한다.
  * 이 인덱스는 오프셋과 세그먼트 파일 및 그 안에서의 위치를 매핑한다.
* 카프카는 타임스탬프와 메시지 오프셋을 매핑하는 또 다른 인덱스를 가지고 있다.
  * 이 인텍스는 타임스탬프를 기준으로 메시지를 찾을 때 사용된다.
* 인덱스들 역시 세그먼트 단위로 분할된다.
  * 따라서 메시지를 삭제할 때 오래 된 인덱스 항목 역시 삭제할 수 있다.

### 6.5.6 압착
* 카프카는 두 가지 보존 정책을 허용한다.
  * 삭제 보존 정책에서는 지정된 보존 기한보다 더 오래 된 이벤트들을 삭제한다.
  * 압착 보존 정책에서는 토픽에서 각 키의 가장 최근값만 저장하도록 한다. 토픽에 키값이 null인 메시지가 있을 경우 압착은 실패한다.
* 보존 기한과 압착 설정을 동시에 적용하도록 설정할 수도 있다.

### 6.5.7 압착의 작동 원리
* 각 로그는 다음과 같이 두 영역으로 나누어 진다.
* 클린(clean)
  * 이전에 압착된 적이 있었던 메시지들이 저장된다. 이 영역은 하나의 키마다 하나의 값만을 포함한다.
* 더티(dirty)
  * 마지막 압착 작업 이후 쓰여진 메시지들이 저장된다.
* 압착 기능이 활성화되어 있을 경우, 각 브로커는 압착 매니저 스레드와 함께 다수의 압착 스레드를 시작시킨다.
  * 압착 스레드들은 압착 작업을 담당한다.
  * 각 스레드는 전체 파티션 크기 대비 더티 메시지의 비율이 가장 높은 파티션을 골라 압착한 뒤 클린 상태로 만든다.

### 6.5.8 삭제된 이벤트
* 특정 키를 갖는 모든 메시지를 삭제하려면 해당 키값과 null 밸류값을 갖는 메시지를 써주면 된다.
* 클리너 스레드가 이 메시지를 발견하면 평소대로 압착 작업을 한 뒤 null 밸류값을 갖는 메시지만을 보존하게 될 것이다.
* 카프카는 사전에 설정된 기간동안 이 특별한 메시지(툼스톤, tombstone 이라고 부른다)를 보존할 것이다.
  * 이 기간 동안, 컨슈머는 이 메시지를 보고 해당 밸류가 삭제되었음을 알 수 있을 것이다.
  * 이 기간이 지나면, 클리너 스레드는 툼스톤 메시지를 삭제하며, 해당 키 역시 카프카 파티션에서 완전히 삭제된다.

### 6.5.9 토픽은 언제 압착되는가?
* 삭제 정책이 현재의 액티브 세그먼트를 절대 삭제하지 않는 것과 마찬가지고, 압착 정책 역시 현재의 액티브 세그먼트를 절대로 압착하지 않는다.
  * 액티브 세그먼트가 아닌 세그먼트에 저장되어 있는 메시지만이 압착 대상이 된다.
* 기본적으로 토픽 내용물의 50% 이상이 더티 레코드인 경우에만 압착을 시작한다.
  * 압착 기능의 목표는 토픽을 지나치게 자주 압착하지 않으면서, 너무 많은 더티 레코드가 존재하지 않도록 하는 것이다.
  * 압착은 토픽의 읽기/쓰기 성능에 영향을 줄 수 있다. 반대로, 너무 많은 더티 레코드는 많은 디스크 공간을 차지한다.
* min.compaction.lag.ms
  * 메시지가 쓰여진 뒤 압착이 될때까지 지나가야 하는 최소 시간을 지정한다.
* max.compaction.lag.ms
  * 메시지가 쓰여진 뒤 압착이 가능해질 때까지 딜레이 될 수 있는 최대 식나을 지정한다.
  * 이 설정은 특정 기한 안에 압착이 반드시 실행된다는 것을 보장해야 하는 상황에서 자주 사용된다.
